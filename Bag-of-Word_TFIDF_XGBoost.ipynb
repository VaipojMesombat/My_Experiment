{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9OvEeKu0xgG"
      },
      "source": [
        "<p>This program will use Bag-of-Word, TFIDF with XGBoost model</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TSOXfpAxtt--"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import linear_model\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy\n",
        "from sklearn.metrics import log_loss\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "from pandas import MultiIndex, Int64Index\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "bgz_AHhCtt_C",
        "outputId": "789af593-1c36-4a71-b7c9-4296f01b0431"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  id  qid1  qid2  \\\n",
              "0           0   0     1     2   \n",
              "1           1   1     3     4   \n",
              "2           2   2     5     6   \n",
              "3           3   3     7     8   \n",
              "4           4   4     9    10   \n",
              "\n",
              "                                           question1  \\\n",
              "0  What is the step by step guide to invest in sh...   \n",
              "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
              "2  How can I increase the speed of my internet co...   \n",
              "3  Why am I mentally very lonely? How can I solve...   \n",
              "4  Which one dissolve in water quikly sugar, salt...   \n",
              "\n",
              "                                           question2  is_duplicate  \n",
              "0  What is the step by step guide to invest in sh...             0  \n",
              "1  What would happen if the Indian government sto...             0  \n",
              "2  How can Internet speed be increased by hacking...             0  \n",
              "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
              "4            Which fish would survive in salt water?             0  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('quora_train.csv')\n",
        "df = df.dropna(how=\"any\").reset_index(drop=True)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hfA3RDugtt_E",
        "outputId": "df5220b4-65f0-4c02-8ccc-07362fe0039a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:xlabel='is_duplicate'>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEECAYAAAA2xHO4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAS2ElEQVR4nO3df6zd9X3f8eerOGFkBGbAQa4NNQqOVmAqHZZByX6wMdm0nQbdQL3ZFLzWmjsEUiO126DSRBZmCbS1aGyDigiPH+oCjCTDavlRF9Jl0ZjhkrgxhlJfBRIcW+DOHqGrYDN574/zuenxzbmfe32vfS/Gz4d0dL73/f18PudzpGu//P18vuc4VYUkSdP5scWegCTp/c2gkCR1GRSSpC6DQpLUZVBIkroMCklS15LFnsDRdtZZZ9WqVasWexqSdFx54YUX/qSqlo0694ELilWrVjE+Pr7Y05Ck40qS70x3zqUnSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkro+cB+4O16suul3F3sKHyiv3fZziz0F6QNrxiuKJOck+WqSl5PsSvIrrf65JN9LsqM9fnaoz81JJpK8kmT9UP2SJDvbuTuTpNVPTvJwq29Psmqoz4Yku9tjw1F995KkGc3miuIQ8KtV9Y0kHwVeSLKtnbujqv7tcOMkFwBjwIXAjwO/n+QTVfUecDewCfifwOPAlcATwEbgYFWdn2QMuB34hSRnALcAa4Bqr721qg7O721LkmZrxiuKqtpXVd9ox28DLwMrOl2uAh6qqner6lVgAlibZDlwWlU9W4P/qPsB4OqhPve340eBK9rVxnpgW1UdaOGwjUG4SJIWyBFtZrcloZ8GtrfSjUm+lWRLkqWttgJ4fajbnlZb0Y6n1g/rU1WHgLeAMztjSZIWyKyDIsmpwJeAz1bV9xksI30cuBjYB/zGZNMR3atTn2uf4bltSjKeZHz//v29tyFJOkKzCookH2IQEr9dVV8GqKo3quq9qvoB8AVgbWu+BzhnqPtKYG+rrxxRP6xPkiXA6cCBzliHqap7qmpNVa1Ztmzk16lLkuZoNnc9BbgXeLmqfnOovnyo2c8DL7bjrcBYu5PpPGA18FxV7QPeTnJZG/M64LGhPpN3NF0DPNP2MZ4C1iVZ2pa21rWaJGmBzOaup08BnwF2JtnRar8OfDrJxQyWgl4DfhmgqnYleQR4icEdUze0O54ArgfuA05hcLfTE61+L/BgkgkGVxJjbawDSW4Fnm/tPl9VB+byRiVJczNjUFTV1xm9V/B4p89mYPOI+jhw0Yj6O8C104y1Bdgy0zwlSceGX+EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6ZgyKJOck+WqSl5PsSvIrrX5Gkm1JdrfnpUN9bk4ykeSVJOuH6pck2dnO3ZkkrX5ykodbfXuSVUN9NrTX2J1kw1F995KkGc3miuIQ8KtV9ZPAZcANSS4AbgKerqrVwNPtZ9q5MeBC4ErgriQntbHuBjYBq9vjylbfCBysqvOBO4Db21hnALcAlwJrgVuGA0mSdOzNGBRVta+qvtGO3wZeBlYAVwH3t2b3A1e346uAh6rq3ap6FZgA1iZZDpxWVc9WVQEPTOkzOdajwBXtamM9sK2qDlTVQWAbfx4ukqQFcER7FG1J6KeB7cDZVbUPBmECfKw1WwG8PtRtT6utaMdT64f1qapDwFvAmZ2xJEkLZNZBkeRU4EvAZ6vq+72mI2rVqc+1z/DcNiUZTzK+f//+ztQkSUdqVkGR5EMMQuK3q+rLrfxGW06iPb/Z6nuAc4a6rwT2tvrKEfXD+iRZApwOHOiMdZiquqeq1lTVmmXLls3mLUmSZmk2dz0FuBd4uap+c+jUVmDyLqQNwGND9bF2J9N5DDatn2vLU28nuayNed2UPpNjXQM80/YxngLWJVnaNrHXtZokaYEsmUWbTwGfAXYm2dFqvw7cBjySZCPwXeBagKraleQR4CUGd0zdUFXvtX7XA/cBpwBPtAcMgujBJBMMriTG2lgHktwKPN/afb6qDsztrUqS5mLGoKiqrzN6rwDgimn6bAY2j6iPAxeNqL9DC5oR57YAW2aapyTp2PCT2ZKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1zRgUSbYkeTPJi0O1zyX5XpId7fGzQ+duTjKR5JUk64fqlyTZ2c7dmSStfnKSh1t9e5JVQ302JNndHhuO2ruWJM3abK4o7gOuHFG/o6oubo/HAZJcAIwBF7Y+dyU5qbW/G9gErG6PyTE3Ager6nzgDuD2NtYZwC3ApcBa4JYkS4/4HUqS5mXGoKiqrwEHZjneVcBDVfVuVb0KTABrkywHTquqZ6uqgAeAq4f63N+OHwWuaFcb64FtVXWgqg4C2xgdWJKkY2g+exQ3JvlWW5qa/Jf+CuD1oTZ7Wm1FO55aP6xPVR0C3gLO7IwlSVpAS+bY727gVqDa828AvwRkRNvq1Jljn8Mk2cRgWYtzzz23N29Js7Dqpt9d7Cl8YLx2288t9hTmbU5XFFX1RlW9V1U/AL7AYA8BBv/qP2eo6Upgb6uvHFE/rE+SJcDpDJa6phtr1Hzuqao1VbVm2bJlc3lLkqRpzCko2p7DpJ8HJu+I2gqMtTuZzmOwaf1cVe0D3k5yWdt/uA54bKjP5B1N1wDPtH2Mp4B1SZa2pa11rSZJWkAzLj0l+SJwOXBWkj0M7kS6PMnFDJaCXgN+GaCqdiV5BHgJOATcUFXvtaGuZ3AH1SnAE+0BcC/wYJIJBlcSY22sA0luBZ5v7T5fVbPdVJckHSUzBkVVfXpE+d5O+83A5hH1ceCiEfV3gGunGWsLsGWmOUqSjh0/mS1J6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkrpmDIokW5K8meTFodoZSbYl2d2elw6duznJRJJXkqwfql+SZGc7d2eStPrJSR5u9e1JVg312dBeY3eSDUftXUuSZm02VxT3AVdOqd0EPF1Vq4Gn288kuQAYAy5sfe5KclLrczewCVjdHpNjbgQOVtX5wB3A7W2sM4BbgEuBtcAtw4EkSVoYMwZFVX0NODClfBVwfzu+H7h6qP5QVb1bVa8CE8DaJMuB06rq2aoq4IEpfSbHehS4ol1trAe2VdWBqjoIbONHA0uSdIzNdY/i7KraB9CeP9bqK4DXh9rtabUV7Xhq/bA+VXUIeAs4szOWJGkBHe3N7IyoVac+1z6Hv2iyKcl4kvH9+/fPaqKSpNmZa1C80ZaTaM9vtvoe4JyhdiuBva2+ckT9sD5JlgCnM1jqmm6sH1FV91TVmqpas2zZsjm+JUnSKHMNiq3A5F1IG4DHhupj7U6m8xhsWj/XlqfeTnJZ23+4bkqfybGuAZ5p+xhPAeuSLG2b2OtaTZK0gJbM1CDJF4HLgbOS7GFwJ9JtwCNJNgLfBa4FqKpdSR4BXgIOATdU1XttqOsZ3EF1CvBEewDcCzyYZILBlcRYG+tAkluB51u7z1fV1E11SdIxNmNQVNWnpzl1xTTtNwObR9THgYtG1N+hBc2Ic1uALTPNUZJ07PjJbElSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK65hUUSV5LsjPJjiTjrXZGkm1JdrfnpUPtb04ykeSVJOuH6pe0cSaS3JkkrX5ykodbfXuSVfOZryTpyB2NK4q/VVUXV9Wa9vNNwNNVtRp4uv1MkguAMeBC4ErgriQntT53A5uA1e1xZatvBA5W1fnAHcDtR2G+kqQjcCyWnq4C7m/H9wNXD9Ufqqp3q+pVYAJYm2Q5cFpVPVtVBTwwpc/kWI8CV0xebUiSFsZ8g6KA30vyQpJNrXZ2Ve0DaM8fa/UVwOtDffe02op2PLV+WJ+qOgS8BZw5zzlLko7Aknn2/1RV7U3yMWBbkj/qtB11JVCdeq/P4QMPQmoTwLnnntufsSTpiMzriqKq9rbnN4GvAGuBN9pyEu35zdZ8D3DOUPeVwN5WXzmiflifJEuA04EDI+ZxT1Wtqao1y5Ytm89bkiRNMeegSPIXk3x08hhYB7wIbAU2tGYbgMfa8VZgrN3JdB6DTevn2vLU20kua/sP103pMznWNcAzbR9DkrRA5rP0dDbwlba3vAT4z1X1ZJLngUeSbAS+C1wLUFW7kjwCvAQcAm6oqvfaWNcD9wGnAE+0B8C9wINJJhhcSYzNY76SpDmYc1BU1beBnxpR/1/AFdP02QxsHlEfBy4aUX+HFjSSpMXhJ7MlSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jougiLJlUleSTKR5KbFno8knUje90GR5CTgPwI/A1wAfDrJBYs7K0k6cbzvgwJYC0xU1ber6v8CDwFXLfKcJOmEcTwExQrg9aGf97SaJGkBLFnsCcxCRtTqsAbJJmBT+/FPk7xyzGd14jgL+JPFnsRMcvtiz0CL5H3/+3kc/W7+xHQnjoeg2AOcM/TzSmDvcIOquge4ZyEndaJIMl5VaxZ7HtIo/n4ujONh6el5YHWS85J8GBgDti7ynCTphPG+v6KoqkNJbgSeAk4CtlTVrkWeliSdMN73QQFQVY8Djy/2PE5QLunp/czfzwWQqpq5lSTphHU87FFIkhaRQSFJ6jou9ii0cJL8ZQaffF/B4PMqe4GtVfXyok5M0qLxikI/lORfMPiKlADPMbg1OcAX/TJGvZ8l+cXFnsMHmZvZ+qEkfwxcWFX/b0r9w8Cuqlq9ODOT+pJ8t6rOXex5fFC59KRhPwB+HPjOlPrydk5aNEm+Nd0p4OyFnMuJxqDQsM8CTyfZzZ9/EeO5wPnAjYs1Kak5G1gPHJxSD/A/Fn46Jw6DQj9UVU8m+QSDr3ZfweAP4B7g+ap6b1EnJ8HvAKdW1Y6pJ5L8wYLP5gTiHoUkqcu7niRJXQaFJKnLoJAkdRkUOmElmdedMkn+cZL/MI/+ryU5az5zSXJ1kgvmOgdpNgwKnbCq6pOLPYdJ85jL1YBBoWPKoNAJK8mftuflSb6WZEeSF5P89U6fX0zyx0n+G/Cpofp9Sa4ZMfblbeyvJHkpyW8l+ZE/d5Pt2/E/T7IzyR8mua3V/kmS51vtS0k+kuSTwN8D/k2b+8fb48kkLyT57+27u6R58XMUEvxD4Kmq2pzkJOAjoxolWQ78K+AS4C3gq8A3ZzH+Wgb/6v8O8CTw94FHp3mNn2FwlXBpVf1ZkjPaqS9X1Rdam38NbKyqf59kK/A7VfVoO/c08E+raneSS4G7gL89izlK0zIopMGXH25J8iHgv476QFdzKfAHVbUfIMnDwCdmMf5zVfXt1ueLwF9jmqAA/g7wn6rqzwCq6kCrX9QC4i8BpzL4r4EPk+RU4JPAf0kyWT55FvOTulx60gmvqr4G/A3ge8CDSa7rNZ+mfoj25ymDv6U/3OnT+5Rrpjl/H3BjVf0VBlc1f2FEmx8D/ndVXTz0+MnOa0mzYlDohJfkJ4A329LOvcBfnabpduDyJGe2q49rh869xmBJCgb/n8eHhs6tTXJe25v4BeDrnen8HvBLST7S5ja59PRRYF973X801P7tdo6q+j7wapJrW98k+anOa0mzYlBIcDmwI8k3gX8A/LtRjapqH/A54Fng94FvDJ3+AvA3kzzHYInq/wydexa4DXgReBX4ynQTqaonga3AeJIdwK+1U/+SQVBtA/5oqMtDwD9L8s0kH2cQIhuT/CGwi0FoSfPidz1Jx1CSy4Ffq6q/u8hTkebMKwpJUpdXFNIISbbzo3cMfaaqdi7GfKTFZFBIkrpcepIkdRkUkqQug0KS1GVQSJK6DApJUtf/ByjhQEQJMKTaAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "df.groupby(\"is_duplicate\")['id'].count().plot.bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xfY8pl9Ftt_G"
      },
      "outputs": [],
      "source": [
        "df.drop(['id', 'qid1', 'qid2'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlp8OQyFtt_H",
        "outputId": "53f37a96-bfb2-4c9d-b0ba-4eb7be1448fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the step by step guide to invest in share market in india?\n",
            "What is the step by step guide to invest in share market?\n",
            "\n",
            "What is the story of Kohinoor (Koh-i-Noor) Diamond?\n",
            "What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\n",
            "\n",
            "How can I increase the speed of my internet connection while using a VPN?\n",
            "How can Internet speed be increased by hacking through DNS?\n",
            "\n",
            "Why am I mentally very lonely? How can I solve it?\n",
            "Find the remainder when [math]23^{24}[/math] is divided by 24,23?\n",
            "\n",
            "Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?\n",
            "Which fish would survive in salt water?\n",
            "\n",
            "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
            "I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\n",
            "\n",
            "Should I buy tiago?\n",
            "What keeps childern active and far from phone and video games?\n",
            "\n",
            "How can I be a good geologist?\n",
            "What should I do to be a great geologist?\n",
            "\n",
            "When do you use シ instead of し?\n",
            "When do you use \"&\" instead of \"and\"?\n",
            "\n",
            "Motorola (company): Can I hack my Charter Motorolla DCX3400?\n",
            "How do I hack Motorola DCX3400 for free internet?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "a = 0 \n",
        "for i in range(a,a+10):\n",
        "    print(df.question1[i])\n",
        "    print(df.question2[i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aHNBhFmEtt_I"
      },
      "outputs": [],
      "source": [
        "SPECIAL_TOKENS = {\n",
        "    'quoted': 'quoted_item',\n",
        "    'non-ascii': 'non_ascii_word',\n",
        "    'undefined': 'something'\n",
        "}\n",
        "\n",
        "def clean(text, stem_words=True):\n",
        "    import re\n",
        "    from string import punctuation\n",
        "    from nltk.stem import SnowballStemmer\n",
        "    from nltk.corpus import stopwords\n",
        "    \n",
        "    def pad_str(s):\n",
        "        return ' '+s+' '\n",
        "    \n",
        "    if pd.isnull(text):\n",
        "        return ''\n",
        "\n",
        "#    stops = set(stopwords.words(\"english\"))\n",
        "    # Clean the text, with the option to stem words.\n",
        "    \n",
        "    # Empty question\n",
        "    \n",
        "    if type(text) != str or text=='':\n",
        "        return ''\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(\"\\'s\", \" \", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
        "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(\"can't\", \"can not\", text)\n",
        "    text = re.sub(\"n't\", \" not \", text)\n",
        "    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'re\", \" are \", text)\n",
        "    text = re.sub(\"\\'d\", \" would \", text)\n",
        "    text = re.sub(\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"b\\.g\\.\", \" bg \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"(\\d+)(kK)\", \" \\g<1>000 \", text)\n",
        "    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"(the[\\s]+|The[\\s]+)?U\\.S\\.A\\.\", \" America \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"(the[\\s]+|The[\\s]+)?United State(s)?\", \" America \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\(s\\)\", \" \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"[c-fC-F]\\:\\/\", \" disk \", text)\n",
        "    \n",
        "    # remove comma between numbers, i.e. 15,000 -> 15000\n",
        "    \n",
        "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
        "    \n",
        "#     # all numbers should separate from words, this is too aggressive\n",
        "    \n",
        "#     def pad_number(pattern):\n",
        "#         matched_string = pattern.group(0)\n",
        "#         return pad_str(matched_string)\n",
        "#     text = re.sub('[0-9]+', pad_number, text)\n",
        "    \n",
        "    # add padding to punctuations and special chars, we still need them later\n",
        "    \n",
        "    text = re.sub('\\$', \" dollar \", text)\n",
        "    text = re.sub('\\%', \" percent \", text)\n",
        "    text = re.sub('\\&', \" and \", text)\n",
        "    \n",
        "#    def pad_pattern(pattern):\n",
        "#        matched_string = pattern.group(0)\n",
        "#       return pad_str(matched_string)\n",
        "#    text = re.sub('[\\!\\?\\@\\^\\+\\*\\/\\,\\~\\|\\`\\=\\:\\;\\.\\#\\\\\\]', pad_pattern, text) \n",
        "        \n",
        "    text = re.sub('[^\\x00-\\x7F]+', pad_str(SPECIAL_TOKENS['non-ascii']), text) # replace non-ascii word with special word\n",
        "    \n",
        "    # indian dollar\n",
        "    \n",
        "    text = re.sub(\"(?<=[0-9])rs \", \" rs \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" rs(?=[0-9])\", \" rs \", text, flags=re.IGNORECASE)\n",
        "    \n",
        "    # clean text rules get from : https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
        "    text = re.sub(r\" (the[\\s]+|The[\\s]+)?US(A)? \", \" America \", text)\n",
        "    text = re.sub(r\" UK \", \" England \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" india \", \" India \", text)\n",
        "    text = re.sub(r\" switzerland \", \" Switzerland \", text)\n",
        "    text = re.sub(r\" china \", \" China \", text)\n",
        "    text = re.sub(r\" chinese \", \" Chinese \", text) \n",
        "    text = re.sub(r\" imrovement \", \" improvement \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" intially \", \" initially \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" quora \", \" Quora \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" dms \", \" direct messages \", text, flags=re.IGNORECASE)  \n",
        "    text = re.sub(r\" demonitization \", \" demonetization \", text, flags=re.IGNORECASE) \n",
        "    text = re.sub(r\" actived \", \" active \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" kms \", \" kilometers \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" cs \", \" computer science \", text, flags=re.IGNORECASE) \n",
        "    text = re.sub(r\" upvote\", \" up vote\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" iPhone \", \" phone \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" \\0rs \", \" rs \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" calender \", \" calendar \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" ios \", \" operating system \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" gps \", \" GPS \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" gst \", \" GST \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" programing \", \" programming \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" bestfriend \", \" best friend \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" dna \", \" DNA \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" III \", \" 3 \", text)\n",
        "    text = re.sub(r\" banglore \", \" Banglore \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" J K \", \" JK \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\" J\\.K\\. \", \" JK \", text, flags=re.IGNORECASE)\n",
        "    \n",
        "    # replace the float numbers with a random number, it will be parsed as number afterward, and also been replaced with word \"number\"\n",
        "    \n",
        "    text = re.sub('[0-9]+\\.[0-9]+', \" 87 \", text)\n",
        "  \n",
        "    \n",
        "    # Remove punctuation from text\n",
        "    text = ''.join([c for c in text if c not in punctuation]).lower()\n",
        "       # Return a list of words\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gCIQMh14tt_N"
      },
      "outputs": [],
      "source": [
        "df['question1'] = df['question1'].apply(clean)\n",
        "df['question2'] = df['question2'].apply(clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP5FTpYytt_P",
        "outputId": "a5ceda86-f2bc-4da1-95bf-1272cfaaceed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "what is the step by step guide to invest in share market in india\n",
            "what is the step by step guide to invest in share market\n",
            "\n",
            "what is the story of kohinoor kohinoor diamond\n",
            "what would happen if the indian government stole the kohinoor kohinoor diamond back\n",
            "\n",
            "how can i increase the speed of my internet connection while using a vpn\n",
            "how can internet speed be increased by hacking through dns\n",
            "\n",
            "why am i mentally very lonely how can i solve it\n",
            "find the remainder when math2324math is divided by 2423\n",
            "\n",
            "which one dissolve in water quikly sugar salt methane and carbon di oxide\n",
            "which fish would survive in salt water\n",
            "\n",
            "astrology i am a capricorn sun cap moon and cap risingwhat does that say about me\n",
            "i am a triple capricorn sun moon and ascendant in capricorn what does this say about me\n",
            "\n",
            "should i buy tiago\n",
            "what keeps childern active and far from phone and video games\n",
            "\n",
            "how can i be a good geologist\n",
            "what should i do to be a great geologist\n",
            "\n",
            "when do you use  nonasciiword  instead of  nonasciiword \n",
            "when do you use  and  instead of and\n",
            "\n",
            "motorola company can i hack my charter motorolla dcx3400\n",
            "how do i hack motorola dcx3400 for free internet\n",
            "\n"
          ]
        }
      ],
      "source": [
        "a = 0 \n",
        "for i in range(a,a+10):\n",
        "    print(df.question1[i])\n",
        "    print(df.question2[i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-q2W3dx1EM0"
      },
      "source": [
        "**Feature engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIAXfnnz1Jhw"
      },
      "source": [
        "We are well experienced in classifying single texts for some time — but the ability to accurately model the relationships between texts is relative new. Here I use Panda's concatfunction to concatenate two text objects features from question1 and question2 into one. The scipy.sparse.hstackis used to stack sparse matrices horizontally (column wise)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrdSRyL61ZU8"
      },
      "source": [
        "You may have noticed that we have off a lot work to do in terms of text cleaning. After some inspections, a few tries and ideas from https://www.kaggle.com/currie32/the-importance-of-cleaning-text, I decided to clean the text as follows:\n",
        "* Not to remove stop words, because words like “what”, “which” and “how” may have strong signals.\n",
        "* Not to stem words.\n",
        "* Remove punctuation.\n",
        "* Correct typos.\n",
        "* Change abbreviations to its original terms.\n",
        "* Remove comma between numbers.\n",
        "* Change special chars to words. And so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHzWwYOPtt_Q"
      },
      "source": [
        "### Bag-Of-Words (BOW) + Xgboost Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NzsqjNw1v80"
      },
      "source": [
        "This following strategy (CountVectorizer) is called the Bag of Words. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document. It tokenizes the documents and counts the occurrences of token and return them as a sparse matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCncU8bn1z0w"
      },
      "source": [
        "The Xgboost was used with the following parameters, selected based on the performance from the validation data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "msXWo6UGtt_V"
      },
      "outputs": [],
      "source": [
        "\n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(pd.concat((df['question1'],df['question2'])).unique())\n",
        "trainq1_trans = count_vect.transform(df['question1'].values)\n",
        "trainq2_trans = count_vect.transform(df['question2'].values)\n",
        "labels = df['is_duplicate'].values\n",
        "X = scipy.sparse.hstack((trainq1_trans,trainq2_trans))\n",
        "y = labels\n",
        "X_train,X_valid,y_train,y_valid = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
        "\n",
        "\n",
        "#xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8).fit(X_train, y_train) \n",
        "xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=100, learning_rate=0.1,colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic',eta=0.3, subsample=0.8,eval_metric = 'logloss').fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "xgb_prediction = xgb_model.predict(X_valid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJNEG718tt_W",
        "outputId": "cc333f25-8028-44ad-9796-f70225c902f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training score: 0.8372634482829122\n",
            "validation score: 0.7642749885860887\n",
            "Classclassification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.90      0.85     84267\n",
            "           1       0.78      0.61      0.68     49148\n",
            "\n",
            "    accuracy                           0.79    133415\n",
            "   macro avg       0.79      0.75      0.76    133415\n",
            "weighted avg       0.79      0.79      0.79    133415\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "\n",
        "print('training score:', f1_score(y_train, xgb_model.predict(X_train), average='macro'))\n",
        "print('validation score:', f1_score(y_valid, xgb_model.predict(X_valid), average='macro'))\n",
        "print(\"Classclassification report\") \n",
        "print (classification_report(y_valid, xgb_prediction))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_aVC6-Ltt_X"
      },
      "source": [
        "### Word level TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qCGTeFH2GVx"
      },
      "source": [
        "The tf–idf is the product of two statistics, term frequency and inverse document frequency, it is one of the most popular term-weighting schemes today"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRCepMcB2Hvl"
      },
      "source": [
        "we apply tf-idf normalization to a sparse matrix of occurrence counts, from word level, n-gram level and character level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "23QnOMH0tt_Z"
      },
      "outputs": [],
      "source": [
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(pd.concat((df['question1'],df['question2'])).unique())\n",
        "trainq1_trans = tfidf_vect.transform(df['question1'].values)\n",
        "trainq2_trans = tfidf_vect.transform(df['question2'].values)\n",
        "labels = df['is_duplicate'].values\n",
        "X = scipy.sparse.hstack((trainq1_trans,trainq2_trans))\n",
        "y = labels\n",
        "X_train,X_valid,y_train,y_valid = train_test_split(X,y, test_size = 0.33, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydu-BwyWtt_a",
        "outputId": "7c00d356-f280-4d61-d5b3-1f67a291cd0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word level tf-idf training score: 0.9038570640838578\n",
            "word level tf-idf validation score: 0.769801674148215\n",
            "\t\t Classclassification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.90      0.85     84267\n",
            "           1       0.78      0.62      0.69     49148\n",
            "\n",
            "    accuracy                           0.80    133415\n",
            "   macro avg       0.79      0.76      0.77    133415\n",
            "weighted avg       0.79      0.80      0.79    133415\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "#xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8).fit(X_train, y_train) \n",
        "xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=100, learning_rate=0.1,colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic',eta=0.3, subsample=0.8,eval_metric = 'logloss').fit(X_train, y_train)\n",
        "xgb_prediction = xgb_model.predict(X_valid)\n",
        "\n",
        "print('word level tf-idf training score:', f1_score(y_train, xgb_model.predict(X_train), average='macro'))\n",
        "print('word level tf-idf validation score:', f1_score(y_valid, xgb_model.predict(X_valid), average='macro'))\n",
        "\n",
        "print(\"\\t\\t Classclassification report\") \n",
        "print (classification_report(y_valid, xgb_prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa7zb6QS2Uju"
      },
      "source": [
        "A subtle point about feature engineering is that it requires knowing feature statistics that we most likely do not know in practice. In order to compute the tf-idf representation, we have to compute the inverse document frequencies based on the training data and use these statistics to scale both the training and test data. In scikit-learn, fitting the feature transformer on the training data amounts to collecting the relevant statistics. The fitted transformer can then be applied to the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OLkPiHntt_b"
      },
      "source": [
        "### N-gram Level TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9vETRvNktt_b"
      },
      "outputs": [],
      "source": [
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(pd.concat((df['question1'],df['question2'])).unique())\n",
        "trainq1_trans = tfidf_vect_ngram.transform(df['question1'].values)\n",
        "trainq2_trans = tfidf_vect_ngram.transform(df['question2'].values)\n",
        "labels = df['is_duplicate'].values\n",
        "X = scipy.sparse.hstack((trainq1_trans,trainq2_trans))\n",
        "y = labels\n",
        "X_train,X_valid,y_train,y_valid = train_test_split(X,y, test_size = 0.33, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfCteOnitt_c",
        "outputId": "bf966ab6-6c68-4670-f882-162e323b9a4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n-gram level tf-idf training score: 0.7537922128382912\n",
            "n-gram level tf-idf validation score: 0.6870584595619965\n",
            "\t\t Classclassification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.91      0.82     84267\n",
            "           1       0.75      0.45      0.56     49148\n",
            "\n",
            "    accuracy                           0.74    133415\n",
            "   macro avg       0.74      0.68      0.69    133415\n",
            "weighted avg       0.74      0.74      0.72    133415\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8).fit(X_train, y_train) \n",
        "xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=100, learning_rate=0.1,colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic',eta=0.3, subsample=0.8,eval_metric = 'logloss').fit(X_train, y_train)\n",
        "xgb_prediction = xgb_model.predict(X_valid)\n",
        "\n",
        "print('n-gram level tf-idf training score:', f1_score(y_train, xgb_model.predict(X_train), average='macro'))\n",
        "print('n-gram level tf-idf validation score:', f1_score(y_valid, xgb_model.predict(X_valid), average='macro'))\n",
        "\n",
        "print(\"\\t\\t Classclassification report\") \n",
        "print (classification_report(y_valid, xgb_prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G6RKgBrtt_d"
      },
      "source": [
        "### Character Level TF-IDF "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p1QiP06_tt_e",
        "outputId": "a0e297de-0290-440d-e720-11234b9a2439"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "character level tf-idf training score: 0.9992948048699548\n",
            "character level tf-idf validation score: 0.8086898884524534\n",
            "\t\t Classclassification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87     84267\n",
            "           1       0.82      0.68      0.75     49148\n",
            "\n",
            "    accuracy                           0.83    133415\n",
            "   macro avg       0.83      0.80      0.81    133415\n",
            "weighted avg       0.83      0.83      0.83    133415\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(pd.concat((df['question1'],df['question2'])).unique())\n",
        "trainq1_trans = tfidf_vect_ngram_chars.transform(df['question1'].values)\n",
        "trainq2_trans = tfidf_vect_ngram_chars.transform(df['question2'].values)\n",
        "labels = df['is_duplicate'].values\n",
        "X = scipy.sparse.hstack((trainq1_trans,trainq2_trans))\n",
        "y = labels\n",
        "X_train,X_valid,y_train,y_valid = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
        "#xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8).fit(X_train, y_train) \n",
        "xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=100, learning_rate=0.1,colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic',eta=0.3, subsample=0.8,eval_metric = 'logloss').fit(X_train, y_train)\n",
        "xgb_prediction = xgb_model.predict(X_valid)\n",
        "\n",
        "print('character level tf-idf training score:', f1_score(y_train, xgb_model.predict(X_train), average='macro'))\n",
        "print('character level tf-idf validation score:', f1_score(y_valid, xgb_model.predict(X_valid), average='macro'))\n",
        "\n",
        "print(\"\\t\\t Classclassification report\") \n",
        "print (classification_report(y_valid, xgb_prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BOW_TFIDF_XGBoost.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
